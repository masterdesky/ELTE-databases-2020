%----------------------------------------------------------------------------------------
%	SLIDE 2.
%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Amdahl's law}

\begin{itemize}
	\item<1-> Quantifies the theoretical magnitude of speedup due to parallelization.
	\item<2-> We can split a whole arbitrary algorithm (denoting with $1$) into to parts:
	\begin{equation} \label{eq:1}
		T = S*T + P*T
		\quad \to \quad
		1 = S + P,
	\end{equation}
	where $T$ is the total execution time of the algorithm, $S <= 1$ denotes the fraction of runtime of the sequentially and $P <= 1$ the parallel solved part.
	\item<3-> Sequential part ($S$)
	\begin{itemize}
		\item<3-> Takes a lot of time in every case, because this part is not parallelizable.
	\end{itemize}
	\item<4-> Parallel part ($P$)
	\begin{itemize}
		\item<4-> Doing the same operations in parallel on a lot of batch of data.
		\item<4-> Should be always a lot faster than $S$.
	\end{itemize}
\end{itemize}

\end{frame}